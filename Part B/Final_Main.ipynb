{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e845b30f-b3fb-4aa9-9da2-133176b01e17",
   "metadata": {},
   "source": [
    "# ABSA Model Training\n",
    "\n",
    "In this section, we present the process of training a layer on a pre trained Aspect-Based Sentiment Analysis (ABSA) model using BERT.\n",
    "\n",
    "## Training the ABSA Model\n",
    "\n",
    "1. **Loading the Data:**\n",
    "   - Iterate over the datasets containing normalized poem data.\n",
    "   - Convert the dataset into a suitable format for training.\n",
    "\n",
    "2. **Model Initialization:**\n",
    "   - Initialize the ABSAModel instance with a pre-trained BERT tokenizer.\n",
    "   - Load the pre-trained BERT-based ABSA model.\n",
    "\n",
    "3. **Training Additional Layer:**\n",
    "   - Train the additional layer of the ABSA model on each dataset.\n",
    "   - Iterate over the datasets and train the model while saving the trained model at each iteration.\n",
    "\n",
    "4. **Save the Trained Model:**\n",
    "   - Save the trained model with a specific name corresponding to the iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca97c199-dd9a-49bf-ba5a-f97e3f98a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.insert(1, '../dataset')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from collections import Counter\n",
    "sys.path.append(r'C:\\SVSHARE\\final\\BERT-Aspect-Based-Sentiment-Analysis\\src')\n",
    "from absa import *\n",
    "# from tabulate import tabulate\n",
    "import csv\n",
    "\n",
    "\n",
    "### Load the pre-trained tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "### Initialize your ABSAModel instance\n",
    "absa_model = ABSAModel(tokenizer)\n",
    "### To train additional layer:\n",
    "## please specify the name of the model and the path to the dataset.\n",
    "model_name = 'shakespeare_absa_model_test_first.pth'\n",
    "path_to_training_files = 'C:\\\\SVSHARE\\\\final\\\\BERT-Aspect-Based-Sentiment-Analysis\\\\dataset\\\\normalized\\\\first_25\\\\'\n",
    "files = os.listdir(path_to_training_files)\n",
    "print(files)\n",
    "counter = 0\n",
    "## start iterating over the datasets to train the additional layer over and over\n",
    "for file in files:\n",
    "    if counter != 0:\n",
    "        ## specify the name of the model\n",
    "        absa_model.load_model(model_name)\n",
    "    input_file = path_to_training_files + file\n",
    "    print(input_file)\n",
    "    shakespeare_df = pd.read_csv(input_file)\n",
    "## convert your dataset to a suitable format for the trainer\n",
    "    shakespeare_dataset = ABSADataset(shakespeare_df, tokenizer)\n",
    "## Train only the new layer\n",
    "    absa_model.train_new_layer(shakespeare_dataset, epochs=5, device='cpu', lr=1e-4)\n",
    "## Save the trained model - choose the exact name like at the beggining   of the iteration\n",
    "    absa_model.save_model(model_name)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43f07ec-6c77-4362-9f49-dbfbda0152b3",
   "metadata": {},
   "source": [
    "### Validating the Trained Model\n",
    "\n",
    "1. **Load the Trained Model:**\n",
    "   - Load the trained ABSA model for validation.\n",
    "\n",
    "2. **Prepare Test Data:**\n",
    "   - Load the test dataset containing poems to be evaluated.\n",
    "\n",
    "3. **Test the Model:**\n",
    "   - Test the trained ABSA model on the test dataset.\n",
    "   - Evaluate the model's performance using metrics such as accuracy and loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4a396e-4f8d-4206-84cd-f3ea2529cca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.insert(1, '../dataset')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from collections import Counter\n",
    "sys.path.append(r'C:\\SVSHARE\\final\\BERT-Aspect-Based-Sentiment-Analysis\\src')\n",
    "from absa import *\n",
    "# from tabulate import tabulate\n",
    "import csv\n",
    "\n",
    "### Load the pre-trained tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "### Initialize your ABSAModel instance\n",
    "absa_model = ABSAModel(tokenizer)\n",
    "## to run a test on the trained model\n",
    "# specify the name of the model you would like to test\n",
    "absa_model.load_model('shakespeare_absa_model_first_25.pth')\n",
    "## give the path to the test file\n",
    "test_file = 'C:\\\\SVSHARE\\\\final\\\\BERT-Aspect-Based-Sentiment-Analysis\\\\dataset\\\\normalized\\\\combined_to_test.csv'\n",
    "test_df = pd.read_csv(test_file)\n",
    "## convert your dataset to a suitable format for the test\n",
    "test_dataset = ABSADataset(test_df, tokenizer)\n",
    "## Test the trained model\n",
    "absa_model.test_model(test_dataset, 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26c2827-73ad-4365-bc9b-7ea93621e405",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "\n",
    "1. **Load the Trained Model:**\n",
    "   - Load the trained ABSA model for making predictions.\n",
    "\n",
    "2. **Prepare Prediction Data:**\n",
    "   - Iterate over the dataset of poems to be predicted.\n",
    "\n",
    "3. **Execute Prediction Function:**\n",
    "   - Execute the prediction function on each poem dataset.\n",
    "   - Determine if each poem is written by Shakespeare or not based on the model's predictions.\n",
    "\n",
    "4. **Save Predictions:**\n",
    "   - Write the predictions to a CSV file containing filenames and corresponding predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87635cb6-e413-4d9b-8a32-569a02d509bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.insert(1, '../dataset')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from collections import Counter\n",
    "sys.path.append(r'C:\\SVSHARE\\final\\BERT-Aspect-Based-Sentiment-Analysis\\src')\n",
    "from absa import *\n",
    "# from tabulate import tabulate\n",
    "import csv\n",
    "\n",
    "### Load the pre-trained tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "### Initialize your ABSAModel instance\n",
    "absa_model = ABSAModel(tokenizer)\n",
    "## Function to make predictions\n",
    "def predict_text_writer(dataset):\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=False, collate_fn=absa_model.padding)\n",
    "    absa_model.model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader):\n",
    "            ids_tensors, segments_tensors, masks_tensors, _ = batch\n",
    "            ids_tensors = ids_tensors.to('cpu')\n",
    "            segments_tensors = segments_tensors.to('cpu')\n",
    "            masks_tensors = masks_tensors.to('cpu')\n",
    "\n",
    "            outputs = absa_model.model(ids_tensors=ids_tensors, segments_tensors=segments_tensors, masks_tensors=masks_tensors)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions.extend(predicted.tolist())\n",
    "    # Convert predictions to labels\n",
    "    labels = ['Shakespeare' if pred == 2 else 'Non-Shakespeare' for pred in predictions]\n",
    "    # Count occurrences of Shakespeare and Non-Shakespeare predictions\n",
    "    shakespeare_count = labels.count('Shakespeare')\n",
    "    non_shakespeare_count = labels.count('Non-Shakespeare')\n",
    "    # Determine the majority prediction\n",
    "    print(f\"Number of Shakespeare texts: {shakespeare_count}\")\n",
    "    print(f\"Number of non-Shakespeare texts: {non_shakespeare_count}\")\n",
    "    if shakespeare_count > non_shakespeare_count:\n",
    "            to_table[file] = [\"Poem is written by Shakespeare\"]\n",
    "            return 'Shakespeare'\n",
    "    else:\n",
    "            to_table[file] = [\"Poem is not written by Shakespeare\"]\n",
    "            return 'Non-Shakespeare'\n",
    "### make predictions on the trained model\n",
    "## specify the name of the model you would like to predict on\n",
    "absa_model.load_model('shakespeare_absa_model_first_25.pth')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "## give path to poems dataset\n",
    "folder_path = 'C:\\\\SVSHARE\\\\final\\\\Shakespeare_pred'\n",
    "output_location = 'C:\\\\SVSHARE\\\\final\\\\Shakespeare_prediction.csv'\n",
    "files = os.listdir(folder_path)\n",
    "print(files)\n",
    "to_table = {}\n",
    "for file in files:\n",
    "    csv_file_path = folder_path + \"\\\\\" + file\n",
    "    print(csv_file_path)\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    ## convert your dataset to a suitable format for the prediction\n",
    "    shakespeare_dataset = ABSADataset(df, tokenizer)\n",
    "    \n",
    "    ## execute the prediction function on the converted dataset\n",
    "    shakespeare_predictions = predict_text_writer(shakespeare_dataset)\n",
    "    print(shakespeare_predictions)\n",
    "# Create a list of dictionaries from the to_table dictionary\n",
    "rows = [{'Filename': filename, 'Prediction': prediction} for filename, prediction in to_table.items()]\n",
    "\n",
    "# Write the data to the CSV file\n",
    "with open(output_location, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['Filename', 'Prediction']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
